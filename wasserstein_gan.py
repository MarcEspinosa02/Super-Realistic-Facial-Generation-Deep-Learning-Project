# -*- coding: utf-8 -*-
"""Wasserstein Gan DIV.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_95fTlSkwhUTkkqp3EBS_mVT7pM8JyQs
"""
        
import os
import time
import numpy as np
import matplotlib.pyplot as plt
from PIL import Image
from google.colab import drive
import math
import sys

import torchvision.transforms as transforms
from torchvision.utils import save_image
from torch.utils.data import DataLoader
from torchvision import datasets
from torch.autograd import Variable

import torch.nn as nn
import torch.nn.functional as F
import torch.autograd as autograd
import torch

# Mount Google Drive
drive.mount('/content/drive')
data_path = '/content/drive/Shareddrives/DEEP LEARNING/DeepLearning_2021/Final/Data/'
results_path = '/content/drive/Shareddrives/DEEP LEARNING/DeepLearning_2021/Final/Results/'
IMAGE_PATH = '/content/drive/Shareddrives/DEEP LEARNING/DeepLearning_2021/Final/Data/img_align_celeba.zip'

# Creation of the directory for the data and images unpacking
!mkdir '/content/imgs/'
!unzip -u '/content/drive/Shareddrives/DEEP LEARNING/DeepLearning_2021/Final/Data/img_align_celeba.zip' -d '/content/imgs/'
!ls

# -----------------
#  Configure data loader
# -----------------
tr = transforms.Compose([
               transforms.Resize(64),
               transforms.CenterCrop(64),
               transforms.ToTensor(),
               transforms.Normalize(mean=[0.5],
                            std=[0.5])
                ])

ImageFolder = datasets.ImageFolder('/content/imgs/', tr)

test_size = 50000 #int( 0.4 * len(ImageFolder))
train_size = len(ImageFolder) - test_size
dataA,dataB = torch.utils.data.random_split(ImageFolder, [train_size, test_size]) 


dataloader = torch.utils.data.DataLoader(
    dataB,
    batch_size=64,
    shuffle=True,
    drop_last=True,
    num_workers=2
)

# Cuda Configuration for the GPU usage
cuda = True if torch.cuda.is_available() else False
device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor

# Statement of the images Channels and size
img_shape = (3, 64, 64)

# -----------------
#  Generator class 
# -----------------
class Generator(nn.Module):
    def __init__(self):
        super(Generator, self).__init__()

        def block(in_feat, out_feat, normalize=True):
            layers = [nn.Linear(in_feat, out_feat)]
            if normalize:
                layers.append(nn.BatchNorm1d(out_feat, 0.8))
            layers.append(nn.LeakyReLU(0.2, inplace=True))
            return layers

        self.model = nn.Sequential(
            *block(100, 128, normalize=False),
            *block(128, 256),
            *block(256, 512),
            *block(512, 1024),
            nn.Linear(1024, int(np.prod(img_shape))),
            nn.Tanh()
        )

    def forward(self, z):
        img = self.model(z)
        img = img.view(img.shape[0], *img_shape)
        return img

# -----------------
#  Discriminator class 
# -----------------
class Discriminator(nn.Module):
    def __init__(self):
        super(Discriminator, self).__init__()

        self.model = nn.Sequential(
            nn.Linear(int(np.prod(img_shape)), 512),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Linear(512, 256),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Linear(256, 1),
        )

    def forward(self, img):
        img_flat = img.view(img.shape[0], -1)
        validity = self.model(img_flat)
        return validity


# Initialize generator and discriminator
generator = Generator()
discriminator = Discriminator()

if cuda:
    generator.cuda()
    discriminator.cuda()


# -----------------
#  Optimizers and Hyperparameters
# -----------------
lr_w = 0.00005
b1 = 0.5
b2 = 0.999

optimizer_G = torch.optim.Adam(generator.parameters(), lr=lr_w, betas=(b1, b2))
optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=lr_w, betas=(b1, b2))

# Commented out IPython magic to ensure Python compatibility. 
#  |__ > Prints where commented, if it does not work correctly change the prints in this function, it may change depending on the IDE

# -----------------
#  Training function 
# -----------------
def train(n_epochs = 40, steps_trian = 5, save_path = results_path+'GanDIV/'):
  #Parameters for gradient penalty calculation
    k = 2
    p = 6
    loss_generator_avg = []

    batches_done = 0
    for epoch in range(n_epochs):
        loss_gen_avg = 0
        for i, (imgs, _) in enumerate(dataloader):

            imgs = imgs.to(device)
            # Configure input
            real_imgs = Variable(imgs, requires_grad=True)

            #  Train Discriminator
            optimizer_D.zero_grad()

            # Sample noise as generator input
            z = Variable(Tensor(np.random.normal(0, 1, (imgs.shape[0], 100))))

            # Generate a batch of images
            fake_imgs = generator(z)

            # Fake images
            fake_validity = discriminator(fake_imgs)

            # Real images
            real_validity = discriminator(real_imgs)

            # -----------------
            # Compute W-div gradient penalty
            # -----------------

            real_grad_out = Variable(Tensor(real_imgs.size(0), 1).fill_(1.0), requires_grad=False)
            real_grad = autograd.grad(
                real_validity, real_imgs, real_grad_out, create_graph=True, retain_graph=True, only_inputs=True
            )[0]
            real_grad_norm = real_grad.view(real_grad.size(0), -1).pow(2).sum(1) ** (p / 2)

            fake_grad_out = Variable(Tensor(fake_imgs.size(0), 1).fill_(1.0), requires_grad=False)
            fake_grad = autograd.grad(
                fake_validity, fake_imgs, fake_grad_out, create_graph=True, retain_graph=True, only_inputs=True
            )[0]
            fake_grad_norm = fake_grad.view(fake_grad.size(0), -1).pow(2).sum(1) ** (p / 2)

            div_gp = torch.mean(real_grad_norm + fake_grad_norm) * k / 2

            # Adversarial loss
            d_loss = -torch.mean(real_validity) + torch.mean(fake_validity) + div_gp

            d_loss.backward()
            optimizer_D.step()

            optimizer_G.zero_grad()

            if i % steps_trian == 0:

                # -----------------
                #  Train Generator
                # -----------------

                # Generate a batch of images
                fake_imgs = generator(z)

                # Loss measures generator's ability to fool the discriminator
                # Train on fake images
                fake_validity = discriminator(fake_imgs)
                g_loss = -torch.mean(fake_validity)

                g_loss.backward()
                optimizer_G.step()

                loss_gen_avg += g_loss.cpu().item()

                print(
                    "[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f]" % (epoch, n_epochs, i, len(dataloader), d_loss.item(), g_loss.item())
                )
                # Number of batches to take a sample
                if batches_done % 400 == 0:
                    save_image(fake_imgs.data[:25], save_path+"%d.png" % batches_done, nrow=5, normalize=True)

                batches_done += steps_trian
        loss_generator_avg.append(loss_gen_avg/(len(dataloader)/steps_trian))
        print(
            "--->[Epoch %d/%d] [G loss Average: %f]" % (epoch, n_epochs, (loss_gen_avg/(len(dataloader)/steps_trian)))
        )
    return loss_generator_avg


# -----------------
#  Executing the program
# -----------------

!mkdir '/content/drive/Shareddrives/DEEP LEARNING/DeepLearning_2021/Final/Results/GanDIV_400_50000/'
!mkdir '/content/drive/Shareddrives/DEEP LEARNING/DeepLearning_2021/Final/Results/GanDIV_150_100000/'

start = time.time()
losses_list = train(400,5,results_path+'GanDIV_400_50000/')
end = time.time()
hours, rem = divmod(end-start, 3600)
minutes, seconds = divmod(rem, 60)
print("{:0>2}:{:0>2}:{:05.2f}".format(int(hours),int(minutes),seconds))


